{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e64216f-118b-467b-bae9-2ef79e264f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 01:41:56.284135: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-23 01:41:56.330642: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-23 01:41:57.114529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.transforms import Activations, AsDiscrete, Compose\n",
    "from monai.utils.enums import MetricReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db1b8cd-af16-423b-bcae-f29009e25261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai2lab/workshop_February\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e5bba0-afac-419e-9964-429a22a93a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_dir='/home/ai2lab/extension_to_brats_challenge/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData', output_dir='/home/ai2lab/workshop_February/dataset', save_checkpoint=False, max_epochs=200, batch_size=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Swin UNETR for Automated Brain Tumor Segmentation\")\n",
    "\n",
    "    parser.add_argument(\"--data_dir\", default=\"/home/ai2lab/extension_to_brats_challenge/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\", type=str, help=\"Dataset directory\")\n",
    "    parser.add_argument(\"--output_dir\",default=\"/home/ai2lab/workshop_February/dataset\",type=str, help=\"output directory\")\n",
    "    parser.add_argument(\"--save_checkpoint\", action=\"store_true\", help=\"Save checkpoint during training\")\n",
    "    parser.add_argument(\"--max_epochs\", default=200, type=int, help=\"Max number of training epochs\")\n",
    "    parser.add_argument(\"--batch_size\", default=1, type=int, help=\"Batch size\")\n",
    "\n",
    "    # Detect if running in Jupyter\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        # Jupyter: ignore sys.argv to prevent conflicts\n",
    "        return parser.parse_args(args=[])\n",
    "    else:\n",
    "        # Standard script: parse normally\n",
    "        return parser.parse_args()\n",
    "\n",
    "# Get arguments\n",
    "args = get_args()\n",
    "\n",
    "# Example usage\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece57d98-422c-4771-b2e8-797a2a980153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import MapTransform\n",
    "from monai.transforms import (\n",
    "     ToTensord,\n",
    "    LoadImaged,\n",
    "RandSpatialCropd,\n",
    "NormalizeIntensityd\n",
    ")\n",
    "\n",
    "class ConvertToMultiChannelBasedOnCustomBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Converts label values to multi-channel format for BraTS-like task.\n",
    "    Your dataset label IDs:\n",
    "    - 1: necrosis/NCR\n",
    "    - 2: edema\n",
    "    - 3: enhancing tumor (ET)\n",
    "\n",
    "    Channels:\n",
    "    - Channel 0: Tumor Core (TC) = 1 + 3\n",
    "    - Channel 1: Whole Tumor (WT) = 1 + 2 + 3\n",
    "    - Channel 2: Enhancing Tumor (ET) = 3\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            seg = d[key]  # (C, H, W, D) or (H, W, D)\n",
    "            \n",
    "            if isinstance(seg, torch.Tensor):\n",
    "                seg = seg.numpy()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            # make sure we're working with 3D (no extra channel dim)\n",
    "            if seg.ndim == 4 and seg.shape[0] == 1:\n",
    "                seg = np.squeeze(seg, axis=0)\n",
    "            \n",
    "            seg = np.where(seg == 4, 3, seg)\n",
    "            tc = np.logical_or(seg == 1, seg == 3)   # Tumor Core\n",
    "            wt = np.logical_or(tc, seg == 2)         # Whole Tumor\n",
    "            et = seg == 3                             # Enhancing Tumor\n",
    "\n",
    "            multi_channel = np.stack([tc, wt, et], axis=0).astype(np.float32)  # (3, H, W, D)\n",
    "            d[key] = multi_channel\n",
    "        return d\n",
    "# For training (includes segmentation if available)\n",
    "def print_shape(d):\n",
    "    for k, v in d.items():\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    return d\n",
    "\n",
    "    \n",
    "class LoadNumpyd(MapTransform):\n",
    "    def __init__(self, keys, allow_missing_keys=False):\n",
    "        super().__init__(keys)\n",
    "        self.allow_missing_keys = allow_missing_keys\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key not in d:\n",
    "                if self.allow_missing_keys:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise KeyError(f\"Key '{key}' not found in data and allow_missing_keys=False\")\n",
    "\n",
    "            arr = np.load(d[key])  # (1, 128, 768)\n",
    "            arr = np.squeeze(arr, axis=0)  # (128, 768)\n",
    "            arr = arr.astype(np.float32)\n",
    "\n",
    "            d[key] = arr\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"img\",  \"seg\"], allow_missing_keys=True, ensure_channel_first=True),\n",
    "    LoadNumpyd(keys=[\"text_feature\"],allow_missing_keys=True),\n",
    "    ConvertToMultiChannelBasedOnCustomBratsClassesd(keys=\"seg\", allow_missing_keys=True),\n",
    "    RandSpatialCropd(keys=[\"img\", \"seg\"], roi_size=(128,128,128), random_center=True, random_size=False, allow_missing_keys=True),\n",
    "    NormalizeIntensityd(keys=\"img\", nonzero=True, channel_wise=True),\n",
    "    ToTensord(keys=[\"img\", \"seg\",\"text_feature\"], dtype=torch.float32, allow_missing_keys=True),    \n",
    "])    \n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"img\",  \"seg\"], ensure_channel_first=True,allow_missing_keys=True),\n",
    "    LoadNumpyd(keys=[\"text_feature\"],allow_missing_keys=True),\n",
    "    ConvertToMultiChannelBasedOnCustomBratsClassesd(keys=\"seg\",allow_missing_keys=True),\n",
    "    NormalizeIntensityd(keys=\"img\", nonzero=True, channel_wise=True),\n",
    "    ToTensord(keys=[\"img\", \"seg\",\"text_feature\"],dtype=torch.float32, allow_missing_keys=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87ad10a-afa7-4b67-95b8-fafc22c894ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data copying complete.\n"
     ]
    }
   ],
   "source": [
    "##########preparing dataset############\n",
    "# from pathlib import Path\n",
    "# import shutil\n",
    "# # Modify these paths\n",
    "# data_root = Path(args.data_dir)  # contains BraTS-GoAT-00000, BraTS-GoAT-00001, etc.\n",
    "# output_root = Path(args.output_dir)\n",
    "# imagesTr = output_root / \"imagesTr\"\n",
    "# labelsTr = output_root / \"labelsTr\"\n",
    "\n",
    "# # Create directories\n",
    "# imagesTr.mkdir(parents=True, exist_ok=True)\n",
    "# labelsTr.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Mapping from suffix to nnUNet modality index\n",
    "# modality_map = {\n",
    "#     \"t1\": 0,\n",
    "#     \"t1ce\": 1,\n",
    "#     \"t2\": 2,\n",
    "#     \"flair\": 3\n",
    "# }\n",
    "# for case_dir in data_root.iterdir():\n",
    "#     if not case_dir.is_dir():\n",
    "#         continue\n",
    "#     case_id = case_dir.name  # e.g., BraTS20_Training_001\n",
    "#     print(case_id)\n",
    "#     for mod, idx in modality_map.items():\n",
    "#         src_file = case_dir / f\"{case_id}_{mod}.nii\"\n",
    "#         dst_file = imagesTr / f\"{case_id}_{idx:04d}.nii\"\n",
    "#         if src_file.exists():\n",
    "#             shutil.copy(src_file, dst_file)\n",
    "#                 # print('no need')\n",
    "#         else:\n",
    "#             print(f\"Missing modality file: {src_file}\")\n",
    "\n",
    "#         # Handle label\n",
    "#     label_src = case_dir / f\"{case_id}_seg.nii\"\n",
    "#     label_dst = labelsTr / f\"{case_id}.nii\"\n",
    "#     if label_src.exists():\n",
    "#         shutil.copy(label_src, label_dst)\n",
    "#     else:\n",
    "#         print(f\"Warning: No label found for {case_id}, creating dummy\")\n",
    "            \n",
    "\n",
    "print(\"âœ… Data copying complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6745d8be-5de7-4262-b5c9-0d8fa1bd8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory_name, output_dir):\n",
    "    \"\"\"\n",
    "    Load the dataset's training and validation splits from CSV files\n",
    "    \"\"\"\n",
    "\n",
    "    path = directory_name\n",
    "    label_path=os.path.join(path,'labelsTr')\n",
    "    print(label_path)\n",
    "    text_feature_root=os.path.join(path,'text_data/TextBraTSData')\n",
    "    \n",
    "    print(text_feature_root)\n",
    "    \n",
    "    val_csv_path = os.path.join(path, \"imagesTr/validation_set.csv\")\n",
    "    train_csv_path = os.path.join(path, \"imagesTr/train_set.csv\")\n",
    "    \n",
    "    print('check path:',val_csv_path,train_csv_path)\n",
    "\n",
    "    if os.path.exists(val_csv_path) and os.path.exists(train_csv_path):\n",
    "        print(\"Loading existing splits...\")\n",
    "        \n",
    "        val_data = pd.read_csv(val_csv_path)\n",
    "        train_data = pd.read_csv(train_csv_path)\n",
    "    else:\n",
    "        print(\"If you want to perform new split...\")\n",
    "        all_files = sorted([f for f in os.listdir(path) if f.endswith('.nii.gz')])\n",
    "        subject_ids = sorted(set(re.sub(r'_\\d{4}\\.nii\\.gz$', '', f) for f in all_files))\n",
    "        df = pd.DataFrame({'SubjectID': subject_ids})\n",
    "\n",
    "        train_data, val_data = train_test_split(df, test_size=0.20, random_state=42)\n",
    "\n",
    "        train_data['split'] = 'train'\n",
    "        val_data['split'] = 'val'\n",
    "    \n",
    "\n",
    "        final_df = pd.concat([train_data, val_data]).sort_values(by='SubjectID')\n",
    "        output_csv = os.path.join(output_dir, 'train_val_dataset_split.csv')\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "\n",
    "     \n",
    "        # Save each split separately\n",
    "        train_data.to_csv(train_csv_path, index=False)\n",
    "        val_data.to_csv(val_csv_path, index=False)\n",
    "        \n",
    "\n",
    "    # Build final dataset structure\n",
    "    def get_modality_paths(subject_id):\n",
    "        path1=os.path.join(path,\"imagesTr\")\n",
    "        return [\n",
    "            os.path.join(path1, f\"{subject_id}_0000.nii\"),\n",
    "            os.path.join(path1, f\"{subject_id}_0001.nii\"),\n",
    "            os.path.join(path1, f\"{subject_id}_0002.nii\"),\n",
    "            os.path.join(path1, f\"{subject_id}_0003.nii\")\n",
    "        ]\n",
    "\n",
    "    \n",
    "   \n",
    "    def build_data_list(df_split):\n",
    "        data_list = []\n",
    "        for sid in df_split['SubjectID']:\n",
    "            img_paths = get_modality_paths(sid)\n",
    "            seg_path = os.path.join(label_path, f\"{sid}.nii\")\n",
    "            text_feature_file = os.path.join(text_feature_root, sid, f\"{sid}_flair_text.npy\")\n",
    "            item = {\n",
    "                \"img\": img_paths,\n",
    "                \"subject_id\": sid,\n",
    "                \"seg\": seg_path,\n",
    "                \"text_feature\": text_feature_file            \n",
    "            }\n",
    "    \n",
    "            data_list.append(item)\n",
    "        return data_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    filenames_train = build_data_list(train_data)\n",
    "    filenames_val = build_data_list(val_data)\n",
    "    ds_train = Dataset(data=filenames_train, transform=train_transforms)\n",
    "    ds_val = Dataset(data=filenames_val, transform=val_transforms)\n",
    "    train_loader = DataLoader(ds_train, num_workers=4, batch_size=4, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(ds_val, num_workers=4, batch_size=2, shuffle=False, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    # Example: print first batch from train_loader\n",
    "    # for batch in train_loader:\n",
    "    #     # Check keys in the batch\n",
    "    #     print(\"Batch keys:\", batch.keys())\n",
    "    \n",
    "    #     # Example: image data\n",
    "    #     images = batch[\"img\"]  \n",
    "    #     print(\"images shape:\", images.shape)\n",
    "    #     print(\"images dtype:\", images.dtype)\n",
    "    #     print(\"images device:\", images.device if hasattr(images, \"device\") else \"N/A\")\n",
    "    #     print(\"images min/max/mean:\", images.min().item(), images.max().item(), images.mean().item())\n",
    "    \n",
    "    #     # Example: text features\n",
    "    #     if \"text_feature\" in batch:\n",
    "    #         text_feat = batch[\"text_feature\"]\n",
    "    #         print(\"text_feat shape:\", text_feat.shape)\n",
    "    #         print(\"text_feat dtype:\", text_feat.dtype)\n",
    "    #         print(\"text_feat min/max/mean:\", text_feat.min().item(), text_feat.max().item(), text_feat.mean().item())\n",
    "    \n",
    "    #     # Only inspect the first batch\n",
    "    #     break\n",
    "\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6dc9c5-41b4-46ec-aff7-2f397ef63320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai2lab/workshop_February/dataset/labelsTr\n",
      "/home/ai2lab/workshop_February/dataset/text_data/TextBraTSData\n",
      "check path: /home/ai2lab/workshop_February/dataset/imagesTr/validation_set.csv /home/ai2lab/workshop_February/dataset/imagesTr/train_set.csv\n",
      "Loading existing splits...\n",
      "torch.Size([4, 128, 768]) torch.Size([4, 4, 128, 128, 128]) torch.Size([4, 3, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    " # === Step 1: Load data ===\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import re\n",
    "from monai.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "directory_name=args.output_dir\n",
    "output_dir=args.output_dir\n",
    "train_loader, val_loader= load_data(directory_name,output_dir)\n",
    "for batch in train_loader:\n",
    "        print(batch[\"text_feature\"].shape, batch[\"img\"].shape, batch[\"seg\"].shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5396f95f-ec83-4e37-a357-ec760714f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import re\n",
    "from monai.data import decollate_batch\n",
    "import nibabel as nib\n",
    "from monai.transforms import AsDiscrete, Compose, EnsureType, Activations\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from functools import partial\n",
    "from monai.inferers import sliding_window_inference\n",
    "from functools import partial\n",
    "from monai.losses import DiceLoss\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import FocalLoss\n",
    "\n",
    "def convert_to_single_channel(multi_channel_np: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert BraTS-style one-hot (3, H, W, D) prediction or GT to single-channel label map:\n",
    "        0: Background\n",
    "        1: Tumor Core (TC) [label 1 in GT]\n",
    "        2: Edema [label 2 in GT]\n",
    "        3: Enhancing Tumor (ET) [label 3 in GT]\n",
    "\n",
    "    Assumes:\n",
    "        Channel 0: TC = 1 + 3\n",
    "        Channel 1: WT = 1 + 2 + 3\n",
    "        Channel 2: ET = 3\n",
    "    \"\"\"\n",
    "    assert multi_channel_np.shape[0] == 3, \"Expected 3 channels (TC, WT, ET)\"\n",
    "    \n",
    "    tc = multi_channel_np[0]\n",
    "    et = multi_channel_np[2]\n",
    "\n",
    "    output = np.zeros_like(tc, dtype=np.uint8)\n",
    "\n",
    "    # Priority-based assignment\n",
    "    output[tc == 1] = 1  # TC gets label 1 (includes necrosis and ET)\n",
    "    output[(multi_channel_np[1] == 1) & (tc == 0) & (et == 0)] = 2  # Edema only gets label 2\n",
    "    output[et == 1] = 3  # Enhancing Tumor gets label 3 (overwrites TC if needed)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def train(train_loader, val_loader, model, optimizer, scheduler, max_epochs, directory_name, start_epoch=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)            # Move model to cuda:0\n",
    "    model.train()\n",
    "    results_dir=os.path.join(directory_name,\"results\")\n",
    "    os.makedirs(results_dir,exist_ok=True)\n",
    "\n",
    "    criterion = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
    "    criterion_ce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    post_sigmoid = Activations(sigmoid=True)\n",
    "    post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, get_not_nans=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(directory_name, \"best_model.pth\")\n",
    "    last_model_path = os.path.join(directory_name, \"last_model.pth\")\n",
    "    training_results_dir = os.path.join(directory_name, \"training_results\")\n",
    "    os.makedirs(training_results_dir, exist_ok=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_dice_score=-1.0\n",
    "    if os.path.exists(last_model_path):\n",
    "        checkpoint = torch.load(last_model_path, map_location=device)\n",
    "     \n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        # best_val_loss = checkpoint.get('best_val_loss', float(\"inf\"))\n",
    "        best_dice_score=checkpoint.get('best_dice_score',-1)\n",
    "        start_epoch = checkpoint.get('epoch', 1) + 1\n",
    "        print(f\"Last model loaded. Resuming training from epoch: {start_epoch}\")\n",
    "        print(f\"Resuming with best Dice score: {best_dice_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs + 1):\n",
    "        print(f\"\\nðŸ” Epoch {epoch}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            img = batch[\"img\"].to(device)\n",
    "            seg = batch.get(\"seg\").to(device)\n",
    "            text=batch.get(\"text_feature\").to(device)\n",
    "            B, C, H, W, D = img.shape\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_seg = model(img,text)\n",
    "          \n",
    "            loss_seg = criterion(pred_seg, seg) + criterion_ce(pred_seg, seg)\n",
    "\n",
    "            train_loss += loss_seg.item()\n",
    "            loss_seg.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"âœ… Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # Validation\n",
    "        # ----------------------\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        dice_scores = []\n",
    "        import numpy as np\n",
    "    \n",
    "        affine = np.eye(4)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dice_metric.reset()\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                    img = batch[\"img\"].to(device)\n",
    "                    seg = batch.get(\"seg\").to(device)\n",
    "                    text = batch.get(\"text_feature\").to(device)\n",
    "    \n",
    "        \n",
    "                    # Define a predictor lambda that includes text_feature\n",
    "                    predictor_with_text = lambda x: model(x, text)\n",
    "                    \n",
    "                    # Create a sliding_window_inference instance with this predictor\n",
    "                    model_inferer_with_text = partial(\n",
    "                        sliding_window_inference,\n",
    "                        roi_size=[128,128,128],\n",
    "                        sw_batch_size=2,\n",
    "                        predictor=predictor_with_text,\n",
    "                        overlap=0.7,\n",
    "                    )\n",
    "                    \n",
    "                    # Run inference\n",
    "                    pred_seg = model_inferer_with_text(img)\n",
    "                    # val_output_convert = [post_pred(post_sigmoid(p)) for p in pred_seg]\n",
    "                    # pred_seg = [p for p in zip(val_output_convert)]\n",
    "                    # true_seg = [s for s in zip(seg)]\n",
    "                    pred = post_pred(post_sigmoid(pred_seg))\n",
    "\n",
    "\n",
    "                        \n",
    "                    dice_metric(y_pred=pred, y=seg)\n",
    "                    for i in range(img.shape[0]):\n",
    "                        subject_id = batch[\"subject_id\"][i]\n",
    "\n",
    "            \n",
    "                        img_paths = [os.path.join(directory_name, \"imagesTr\", f\"{subject_id}_0000.nii.gz\")]\n",
    "    \n",
    "                        img_path = img_paths[0]\n",
    "                        save_filename = f\"{subject_id}\"\n",
    "                                \n",
    "                        save_img_path = os.path.join(results_dir, f\"{save_filename}_gt.nii.gz\")\n",
    "                        save_pred_path = os.path.join(results_dir, f\"{save_filename}_pred.nii.gz\")\n",
    "                                \n",
    "                        affine = nib.load(img_path).affine\n",
    "                        pred_np = pred[i].detach().cpu().numpy().astype(np.uint8)\n",
    "                        seg_np = seg[i].detach().cpu().numpy().astype(np.uint8)            \n",
    "                        single_channel_pred = convert_to_single_channel(pred_np)\n",
    "                        single_channel_gt = convert_to_single_channel(seg_np)\n",
    "                                   # Save this single-channel prediction as NIfTI\n",
    "                        nib.save(nib.Nifti1Image(single_channel_pred, affine), save_pred_path)\n",
    "                        nib.save(nib.Nifti1Image(single_channel_gt, affine), save_img_path)\n",
    "              \n",
    "        \n",
    "        per_class_dice, _ = dice_metric.aggregate()\n",
    "        mean_dice = per_class_dice.mean().item()\n",
    "        print(f\"Dice Scores â€” TC: {per_class_dice[0].item():.4f}, \"\n",
    "                  f\"WT: {per_class_dice[1].item():.4f}, ET: {per_class_dice[2].item():.4f}\")\n",
    "        print(f\"Mean Dice: {mean_dice:.4f}\")\n",
    "        \n",
    "        if mean_dice > best_dice_score:\n",
    "                best_dice_score = mean_dice\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"scheduler\": scheduler.state_dict(),\n",
    "                    # \"val_loss\": val_loss,\n",
    "                    \"best_dice_score\": best_dice_score\n",
    "                }, checkpoint_path)\n",
    "                print(\"Best model saved based on Dice score.\")\n",
    "            \n",
    "\n",
    "    \n",
    "     \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            # \"val_loss\": val_loss,\n",
    "            # \"best_val_loss\": best_val_loss,\n",
    "            \"best_dice_score\":best_dice_score\n",
    "        }, last_model_path)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08cdebe3-4738-4525-88a4-030e7db97534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.networks.nets import SwinUNETR  # or your custom SwinUNet\n",
    "from monai.optimizers import generate_param_groups\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.distributed as dist\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "\n",
    "class MultiTaskSwinUNETR_image_text_fusion(nn.Module):\n",
    "    def __init__(self,\n",
    "                 # img_size=(256, 256, 160),\n",
    "                 in_channels=4,\n",
    "                 feature_size=48,\n",
    "                 seg_out_channels=3,\n",
    "                 recon_out_channels=4,\n",
    "                 use_checkpoint=True,\n",
    "                 text_embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = SwinUNETR(\n",
    "            # img_size=img_size,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            feature_size=feature_size,\n",
    "            use_checkpoint=use_checkpoint)\n",
    "\n",
    "        # project BioBERT 768 â†’ Swin feature dim (48)\n",
    "        self.text_proj = nn.Linear(text_embed_dim, feature_size)\n",
    "\n",
    "        # fuse image+text channels: (2C â†’ C)\n",
    "        self.fusion = nn.Conv3d(feature_size * 2, feature_size, kernel_size=1)\n",
    "\n",
    "        # output head\n",
    "        self.seg_head = nn.Conv3d(feature_size, seg_out_channels, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, text_feature):\n",
    "    \n",
    "        # --- Image Backbone ---\n",
    "        img_features = self.backbone(x)  # [B, C, D, H, W]\n",
    "        print(f\"[Backbone] img_features: {img_features.shape}\")\n",
    "\n",
    "        # --- Text Processing ---\n",
    "        text_feature = text_feature.mean(dim=1)\n",
    "        print('check shape:',text_feature.shape)\n",
    "        text_feat = self.text_proj(text_feature)          # [B, 768] â†’ [B, C]\n",
    "        print(f\"[TextProj] text_feat after Linear: {text_feat.shape}\")\n",
    "\n",
    "        text_feat = text_feat.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        print(f\"[TextProj] text_feat after unsqueeze: {text_feat.shape}\")\n",
    "\n",
    "        text_feat = text_feat.expand_as(img_features)     # [B, C, D, H, W]\n",
    "        print(f\"[TextProj] text_feat after expand_as: {text_feat.shape}\")\n",
    "\n",
    "        fused = torch.cat([img_features, text_feat], dim=1)  # [B, 2C, D, H, W]\n",
    "        print(f\"[Fusion] concatenated: {fused.shape}\")\n",
    "\n",
    "        fused = self.fusion(fused)                        # [B, C, D, H, W]\n",
    "        print(f\"[Fusion] after Conv3d fusion: {fused.shape}\")\n",
    "        \n",
    "        # --- Head ---\n",
    "        seg_output = self.seg_head(fused)\n",
    "        print(f\"[SegHead] seg_output: {seg_output.shape}\")\n",
    "\n",
    "\n",
    "        return seg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27f5f39c-3b0b-44b6-acdc-f7998448b217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Epoch 1\n",
      "[Backbone] img_features: torch.Size([4, 48, 128, 128, 128])\n",
      "check shape: torch.Size([4, 768])\n",
      "[TextProj] text_feat after Linear: torch.Size([4, 48])\n",
      "[TextProj] text_feat after unsqueeze: torch.Size([4, 48, 1, 1, 1])\n",
      "[TextProj] text_feat after expand_as: torch.Size([4, 48, 128, 128, 128])\n",
      "[Fusion] concatenated: torch.Size([4, 96, 128, 128, 128])\n",
      "[Fusion] after Conv3d fusion: torch.Size([4, 48, 128, 128, 128])\n",
      "[SegHead] seg_output: torch.Size([4, 3, 128, 128, 128])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 47.51 GiB of which 2.84 GiB is free. Process 71219 has 13.04 GiB memory in use. Including non-PyTorch memory, this process has 30.47 GiB memory in use. Of the allocated memory 28.14 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     25\u001b[39m     train(\n\u001b[32m     26\u001b[39m         train_loader=train_loader,\n\u001b[32m     27\u001b[39m         val_loader=val_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m     )\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m max_epochs=\u001b[32m200\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# === Step 5: Call train() ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirectory_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdirectory_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_loader, val_loader, model, optimizer, scheduler, max_epochs, directory_name, start_epoch)\u001b[39m\n\u001b[32m    124\u001b[39m     loss_seg += criterion_ce(pred_seg, seg)\n\u001b[32m    125\u001b[39m     train_loss += loss_seg.item()\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[43mloss_seg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     optimizer.step()\n\u001b[32m    130\u001b[39m train_loss /= \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fsl/lib/python3.12/site-packages/torch/_tensor.py:616\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[32m    574\u001b[39m \n\u001b[32m    575\u001b[39m \u001b[33;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    613\u001b[39m \u001b[33;03m        used to compute the :attr:`tensors`. Defaults to ``None``.\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m torch.autograd.backward(\n\u001b[32m    626\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    627\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fsl/lib/python3.12/site-packages/torch/overrides.py:1750\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m     warnings.warn(\n\u001b[32m   1743\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1744\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1745\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1746\u001b[39m     )\n\u001b[32m   1748\u001b[39m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[32m   1749\u001b[39m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m result = \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fsl/lib/python3.12/site-packages/monai/data/meta_tensor.py:283\u001b[39m, in \u001b[36mMetaTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    282\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fsl/lib/python3.12/site-packages/torch/_tensor.py:1654\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1651\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1656\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fsl/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fsl/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fsl/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 47.51 GiB of which 2.84 GiB is free. Process 71219 has 13.04 GiB memory in use. Including non-PyTorch memory, this process has 30.47 GiB memory in use. Of the allocated memory 28.14 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # === Step 2: Set up device ===\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # === Step 3: Initialize model ===\n",
    "    \n",
    "\n",
    "    model = MultiTaskSwinUNETR_image_text_fusion(\n",
    "       # img_size=(256, 256, 160),\n",
    "       in_channels=4,\n",
    "       seg_out_channels=3,      # tumor classes\n",
    "       recon_out_channels=4,    # reconstruct all 4 modalities\n",
    "       feature_size=48\n",
    "   )\n",
    "\n",
    "    # print(model) \n",
    "       \n",
    "    # === Step 4: parameter setup ===\n",
    "    start_epoch=1\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=4e-4, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "    max_epochs=200\n",
    "   \n",
    "    # === Step 5: Call train() ===\n",
    "    train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        max_epochs=max_epochs,\n",
    "        directory_name=directory_name,\n",
    "        start_epoch=start_epoch,\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81140964-1be2-4de4-b105-d62b1ffbdd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
