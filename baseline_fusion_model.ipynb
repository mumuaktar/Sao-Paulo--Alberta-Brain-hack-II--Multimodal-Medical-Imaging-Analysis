{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e64216f-118b-467b-bae9-2ef79e264f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.transforms import Activations, AsDiscrete, Compose\n",
    "from monai.utils.enums import MetricReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db1b8cd-af16-423b-bcae-f29009e25261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e5bba0-afac-419e-9964-429a22a93a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_dir='/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData', output_dir='/dataset/output', save_checkpoint=False, max_epochs=200, batch_size=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Swin UNETR for Automated Brain Tumor Segmentation\")\n",
    "\n",
    "    parser.add_argument(\"--data_dir\", default=\"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\", type=str, help=\"Dataset directory\")\n",
    "    parser.add_argument(\"--output_dir\",default=\"/dataset/output\",type=str, help=\"output directory\")\n",
    "    parser.add_argument(\"--save_checkpoint\", action=\"store_true\", help=\"Save checkpoint during training\")\n",
    "    parser.add_argument(\"--max_epochs\", default=200, type=int, help=\"Max number of training epochs\")\n",
    "    parser.add_argument(\"--batch_size\", default=1, type=int, help=\"Batch size\")\n",
    "\n",
    "    # Detect if running in Jupyter\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        # Jupyter: ignore sys.argv to prevent conflicts\n",
    "        return parser.parse_args(args=[])\n",
    "    else:\n",
    "        # Standard script: parse normally\n",
    "        return parser.parse_args()\n",
    "\n",
    "# Get arguments\n",
    "args = get_args()\n",
    "\n",
    "# Example usage\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ece57d98-422c-4771-b2e8-797a2a980153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import MapTransform\n",
    "from monai.transforms import (\n",
    "     ToTensord,\n",
    "    LoadImaged,\n",
    "RandSpatialCropd,\n",
    "NormalizeIntensityd\n",
    ")\n",
    "\n",
    "class ConvertToMultiChannelBasedOnCustomBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Converts label values to multi-channel format for BraTS-like task.\n",
    "    Your dataset label IDs:\n",
    "    - 1: necrosis/NCR\n",
    "    - 2: edema\n",
    "    - 3: enhancing tumor (ET)\n",
    "\n",
    "    Channels:\n",
    "    - Channel 0: Tumor Core (TC) = 1 + 3\n",
    "    - Channel 1: Whole Tumor (WT) = 1 + 2 + 3\n",
    "    - Channel 2: Enhancing Tumor (ET) = 3\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            seg = d[key]  # (C, H, W, D) or (H, W, D)\n",
    "            \n",
    "            if isinstance(seg, torch.Tensor):\n",
    "                seg = seg.numpy()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            # make sure we're working with 3D (no extra channel dim)\n",
    "            if seg.ndim == 4 and seg.shape[0] == 1:\n",
    "                seg = np.squeeze(seg, axis=0)\n",
    "            \n",
    "            seg = np.where(seg == 4, 3, seg)\n",
    "            tc = np.logical_or(seg == 1, seg == 3)   # Tumor Core\n",
    "            wt = np.logical_or(tc, seg == 2)         # Whole Tumor\n",
    "            et = seg == 3                             # Enhancing Tumor\n",
    "\n",
    "            multi_channel = np.stack([tc, wt, et], axis=0).astype(np.float32)  # (3, H, W, D)\n",
    "            d[key] = multi_channel\n",
    "        return d\n",
    "\n",
    "def print_shape(d):\n",
    "    for k, v in d.items():\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    return d\n",
    "\n",
    "    \n",
    "class LoadNumpyd(MapTransform):\n",
    "    def __init__(self, keys, allow_missing_keys=False):\n",
    "        super().__init__(keys)\n",
    "        self.allow_missing_keys = allow_missing_keys\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key not in d:\n",
    "                if self.allow_missing_keys:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise KeyError(f\"Key '{key}' not found in data and allow_missing_keys=False\")\n",
    "\n",
    "            arr = np.load(d[key])  # (1, 128, 768)\n",
    "            arr = np.squeeze(arr, axis=0)  # (128, 768)\n",
    "            arr = arr.astype(np.float32)\n",
    "\n",
    "            d[key] = arr\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"img\",  \"seg\"], allow_missing_keys=True, ensure_channel_first=True),\n",
    "    LoadNumpyd(keys=[\"text_feature\"],allow_missing_keys=True),\n",
    "    ConvertToMultiChannelBasedOnCustomBratsClassesd(keys=\"seg\", allow_missing_keys=True),\n",
    "    # RandSpatialCropd(keys=[\"img\", \"seg\"], roi_size=(), allow_missing_keys=True),\n",
    "    NormalizeIntensityd(keys=\"img\", nonzero=True, channel_wise=True),\n",
    "    ToTensord(keys=[\"img\", \"seg\",\"text_feature\"], dtype=torch.float32, allow_missing_keys=True),    \n",
    "])    \n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"img\",  \"seg\"], ensure_channel_first=True,allow_missing_keys=True),\n",
    "    LoadNumpyd(keys=[\"text_feature\"],allow_missing_keys=True),\n",
    "    ConvertToMultiChannelBasedOnCustomBratsClassesd(keys=\"seg\",allow_missing_keys=True),\n",
    "    NormalizeIntensityd(keys=\"img\", nonzero=True, channel_wise=True),\n",
    "    ToTensord(keys=[\"img\", \"seg\",\"text_feature\"],dtype=torch.float32, allow_missing_keys=True),\n",
    "])\n",
    "test_transforms = Compose([\n",
    "    LoadImaged(keys=[\"img\",  \"seg\"], ensure_channel_first=True,allow_missing_keys=True),\n",
    "    LoadNumpyd(keys=[\"text_feature\"],allow_missing_keys=True),\n",
    "    ConvertToMultiChannelBasedOnCustomBratsClassesd(keys=\"seg\",allow_missing_keys=True),\n",
    "    NormalizeIntensityd(keys=\"img\", nonzero=True, channel_wise=True),\n",
    "    ToTensord(keys=[\"img\", \"seg\",\"text_feature\"],dtype=torch.float32, allow_missing_keys=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87ad10a-afa7-4b67-95b8-fafc22c894ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########preparing dataset############the dataset is already prepared using this code snippet\n",
    "# from pathlib import Path\n",
    "# import shutil\n",
    "# # Modify these paths\n",
    "# data_root = Path(args.data_dir)  # contains BraTS-GoAT-00000, BraTS-GoAT-00001, etc.\n",
    "# output_root = Path(args.output_dir)\n",
    "# imagesTr = output_root / \"imagesTr\"\n",
    "# labelsTr = output_root / \"labelsTr\"\n",
    "\n",
    "# # Create directories\n",
    "# imagesTr.mkdir(parents=True, exist_ok=True)\n",
    "# labelsTr.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Mapping from suffix to nnUNet modality index\n",
    "# modality_map = {\n",
    "#     \"t1\": 0,\n",
    "#     \"t1ce\": 1,\n",
    "#     \"t2\": 2,\n",
    "#     \"flair\": 3\n",
    "# }\n",
    "# for case_dir in data_root.iterdir():\n",
    "#     if not case_dir.is_dir():\n",
    "#         continue\n",
    "#     case_id = case_dir.name  # e.g., BraTS20_Training_001\n",
    "#     print(case_id)\n",
    "#     for mod, idx in modality_map.items():\n",
    "#         src_file = case_dir / f\"{case_id}_{mod}.nii\"\n",
    "#         dst_file = imagesTr / f\"{case_id}_{idx:04d}.nii\"\n",
    "#         if src_file.exists():\n",
    "#             shutil.copy(src_file, dst_file)\n",
    "#                 # print('no need')\n",
    "#         else:\n",
    "#             print(f\"Missing modality file: {src_file}\")\n",
    "\n",
    "#         # Handle label\n",
    "#     label_src = case_dir / f\"{case_id}_seg.nii\"\n",
    "#     label_dst = labelsTr / f\"{case_id}.nii\"\n",
    "#     if label_src.exists():\n",
    "#         shutil.copy(label_src, label_dst)\n",
    "#     else:\n",
    "#         print(f\"Warning: No label found for {case_id}, creating dummy\")\n",
    "            \n",
    "\n",
    "# print(\"Data copying complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea54147-b141-41fa-819e-081bf3812b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_list(df, base_path, label_path, text_feature_root):\n",
    "    def get_modality_paths(subject_id):\n",
    "        img_root = os.path.join(base_path, \"imagesTr\")\n",
    "        return [\n",
    "            os.path.join(img_root, f\"{subject_id}_0000.nii\"),\n",
    "            os.path.join(img_root, f\"{subject_id}_0001.nii\"),\n",
    "            os.path.join(img_root, f\"{subject_id}_0002.nii\"),\n",
    "            os.path.join(img_root, f\"{subject_id}_0003.nii\"),\n",
    "        ]\n",
    "\n",
    "    data_list = []\n",
    "    for sid in df[\"SubjectID\"]:\n",
    "        item = {\n",
    "            \"img\": get_modality_paths(sid),\n",
    "            \"seg\": os.path.join(label_path, f\"{sid}.nii\"),\n",
    "            \"subject_id\": sid,\n",
    "            \"text_feature\": os.path.join(\n",
    "                text_feature_root, sid, f\"{sid}_flair_text.npy\"\n",
    "            ),\n",
    "        }\n",
    "        data_list.append(item)\n",
    "\n",
    "    return data_list\n",
    "from monai.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(\n",
    "    base_path,\n",
    "    split,\n",
    "    transforms,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    General data loader for train / val / test\n",
    "\n",
    "    split: 'train', 'val', or 'test'\n",
    "    \"\"\"\n",
    "\n",
    "    assert split in [\"train\", \"val\", \"test\"], \"Invalid split name\"\n",
    "\n",
    "    label_path = os.path.join(base_path, \"labelsTr\")\n",
    "    text_feature_root = os.path.join(base_path, \"text_data/TextBraTSData\")\n",
    "\n",
    "    csv_map = {\n",
    "        \"train\": \"imagesTr/train_set.csv\",\n",
    "        \"val\": \"imagesTr/validation_set.csv\",\n",
    "        \"test\": \"imagesTr/test_set.csv\",\n",
    "    }\n",
    "\n",
    "    csv_path = os.path.join(base_path, csv_map[split])\n",
    "    print(f\"Loading {split} data from:\", csv_path)\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"{csv_path} not found\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    data_list = build_data_list(\n",
    "        df,\n",
    "        base_path=base_path,\n",
    "        label_path=label_path,\n",
    "        text_feature_root=text_feature_root,\n",
    "    )\n",
    "\n",
    "    dataset = Dataset(data=data_list, transform=transforms)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d6dc9c5-41b4-46ec-aff7-2f397ef63320",
   "metadata": {},
   "outputs": [],
   "source": [
    " # === Step 1: Load data ===\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import re\n",
    "from monai.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "directory_name=args.output_dir\n",
    "output_dir=args.output_dir\n",
    "train_loader = load_data(\n",
    "    base_path=directory_name,\n",
    "    split=\"train\",\n",
    "    transforms=train_transforms,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = load_data(\n",
    "    base_path=directory_name,\n",
    "    split=\"val\",\n",
    "    transforms=val_transforms,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_loader = load_data(\n",
    "    base_path=directory_name,\n",
    "    split=\"test\",\n",
    "    transforms=test_transforms,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "        print(batch[\"text_feature\"].shape, batch[\"img\"].shape, batch[\"seg\"].shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5396f95f-ec83-4e37-a357-ec760714f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import re\n",
    "from monai.data import decollate_batch\n",
    "import nibabel as nib\n",
    "from monai.transforms import AsDiscrete, Compose, EnsureType, Activations\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from functools import partial\n",
    "from monai.inferers import sliding_window_inference\n",
    "from functools import partial\n",
    "from monai.losses import DiceLoss\n",
    "import torch.nn.functional as F\n",
    "from monai.losses import FocalLoss\n",
    "\n",
    "def convert_to_single_channel(multi_channel_np: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert BraTS-style one-hot (3, H, W, D) prediction or GT to single-channel label map:\n",
    "        0: Background\n",
    "        1: Tumor Core (TC) [label 1 in GT]\n",
    "        2: Edema [label 2 in GT]\n",
    "        3: Enhancing Tumor (ET) [label 3 in GT]\n",
    "\n",
    "    Assumes:\n",
    "        Channel 0: TC = 1 + 3\n",
    "        Channel 1: WT = 1 + 2 + 3\n",
    "        Channel 2: ET = 3\n",
    "    \"\"\"\n",
    "    assert multi_channel_np.shape[0] == 3, \"Expected 3 channels (TC, WT, ET)\"\n",
    "    \n",
    "    tc = multi_channel_np[0]\n",
    "    et = multi_channel_np[2]\n",
    "\n",
    "    output = np.zeros_like(tc, dtype=np.uint8)\n",
    "\n",
    "    # Priority-based assignment\n",
    "    output[tc == 1] = 1  # TC gets label 1 (includes necrosis and ET)\n",
    "    output[(multi_channel_np[1] == 1) & (tc == 0) & (et == 0)] = 2  # Edema only gets label 2\n",
    "    output[et == 1] = 3  # Enhancing Tumor gets label 3 (overwrites TC if needed)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def train(train_loader, val_loader, model, optimizer, scheduler, max_epochs, directory_name, start_epoch=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)            # Move model to cuda:0\n",
    "    model.train()\n",
    "    results_dir=os.path.join(directory_name,\"results\")\n",
    "    os.makedirs(results_dir,exist_ok=True)\n",
    "\n",
    "    criterion = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
    "    criterion_ce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    post_sigmoid = Activations(sigmoid=True)\n",
    "    post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, get_not_nans=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(directory_name, \"best_model.pth\")\n",
    "    last_model_path = os.path.join(directory_name, \"last_model.pth\")\n",
    "    training_results_dir = os.path.join(directory_name, \"training_results\")\n",
    "    os.makedirs(training_results_dir, exist_ok=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_dice_score=-1.0\n",
    "    if os.path.exists(last_model_path):\n",
    "        checkpoint = torch.load(last_model_path, map_location=device)\n",
    "     \n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        # best_val_loss = checkpoint.get('best_val_loss', float(\"inf\"))\n",
    "        best_dice_score=checkpoint.get('best_dice_score',-1)\n",
    "        start_epoch = checkpoint.get('epoch', 1) + 1\n",
    "        print(f\"Last model loaded. Resuming training from epoch: {start_epoch}\")\n",
    "        print(f\"Resuming with best Dice score: {best_dice_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs + 1):\n",
    "        print(f\"\\nðŸ” Epoch {epoch}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            img = batch[\"img\"].to(device)\n",
    "            seg = batch.get(\"seg\").to(device)\n",
    "            text=batch.get(\"text_feature\").to(device)\n",
    "            B, C, H, W, D = img.shape\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_seg = model(img,text)\n",
    "          \n",
    "            loss_seg = criterion(pred_seg, seg) + criterion_ce(pred_seg, seg)\n",
    "\n",
    "            train_loss += loss_seg.item()\n",
    "            loss_seg.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # Validation\n",
    "        # ----------------------\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        dice_scores = []\n",
    "        import numpy as np\n",
    "    \n",
    "        affine = np.eye(4)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dice_metric.reset()\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                    img = batch[\"img\"].to(device)\n",
    "                    seg = batch.get(\"seg\").to(device)\n",
    "                    text = batch.get(\"text_feature\").to(device)\n",
    "    \n",
    "        \n",
    "                    # Define a predictor lambda that includes text_feature\n",
    "                    predictor_with_text = lambda x: model(x, text)\n",
    "                    \n",
    "                    # Create a sliding_window_inference instance with this predictor\n",
    "                    model_inferer_with_text = partial(\n",
    "                        sliding_window_inference,\n",
    "                        roi_size=[96,96,96],\n",
    "                        sw_batch_size=1,\n",
    "                        predictor=predictor_with_text,\n",
    "                        overlap=0.5,\n",
    "                    )\n",
    "                    \n",
    "                    # Run inference\n",
    "                    pred_seg = model_inferer_with_text(img)\n",
    "                    # val_output_convert = [post_pred(post_sigmoid(p)) for p in pred_seg]\n",
    "                    # pred_seg = [p for p in zip(val_output_convert)]\n",
    "                    # true_seg = [s for s in zip(seg)]\n",
    "                    pred = post_pred(post_sigmoid(pred_seg))\n",
    "\n",
    "\n",
    "                        \n",
    "                    dice_metric(y_pred=pred, y=seg)\n",
    "                    for i in range(img.shape[0]):\n",
    "                        subject_id = batch[\"subject_id\"][i]\n",
    "\n",
    "            \n",
    "                        img_paths = [os.path.join(directory_name, \"imagesTr\", f\"{subject_id}_0000.nii\")]\n",
    "    \n",
    "                        img_path = img_paths[0]\n",
    "                        save_filename = f\"{subject_id}\"\n",
    "                                \n",
    "                        save_img_path = os.path.join(results_dir, f\"{save_filename}_gt.nii\")\n",
    "                        save_pred_path = os.path.join(results_dir, f\"{save_filename}_pred.nii\")\n",
    "                                \n",
    "                        affine = nib.load(img_path).affine\n",
    "                        pred_np = pred[i].detach().cpu().numpy().astype(np.uint8)\n",
    "                        seg_np = seg[i].detach().cpu().numpy().astype(np.uint8)            \n",
    "                        single_channel_pred = convert_to_single_channel(pred_np)\n",
    "                        single_channel_gt = convert_to_single_channel(seg_np)\n",
    "                                   # Save this single-channel prediction as NIfTI\n",
    "                        nib.save(nib.Nifti1Image(single_channel_pred, affine), save_pred_path)\n",
    "                        nib.save(nib.Nifti1Image(single_channel_gt, affine), save_img_path)\n",
    "              \n",
    "        \n",
    "        per_class_dice, _ = dice_metric.aggregate()\n",
    "        mean_dice = per_class_dice.mean().item()\n",
    "        print(f\"Dice Scores â€” TC: {per_class_dice[0].item():.4f}, \"\n",
    "                  f\"WT: {per_class_dice[1].item():.4f}, ET: {per_class_dice[2].item():.4f}\")\n",
    "        print(f\"Mean Dice: {mean_dice:.4f}\")\n",
    "        \n",
    "        if mean_dice > best_dice_score:\n",
    "                best_dice_score = mean_dice\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"scheduler\": scheduler.state_dict(),\n",
    "                    # \"val_loss\": val_loss,\n",
    "                    \"best_dice_score\": best_dice_score\n",
    "                }, checkpoint_path)\n",
    "                print(\"Best model saved based on Dice score.\")\n",
    "            \n",
    "\n",
    "    \n",
    "     \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            # \"val_loss\": val_loss,\n",
    "            # \"best_val_loss\": best_val_loss,\n",
    "            \"best_dice_score\":best_dice_score\n",
    "        }, last_model_path)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08cdebe3-4738-4525-88a4-030e7db97534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.networks.nets import SwinUNETR  # or your custom SwinUNet\n",
    "from monai.optimizers import generate_param_groups\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.distributed as dist\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "\n",
    "class MultiTaskSwinUNETR_image_text_fusion(nn.Module):\n",
    "    def __init__(self,\n",
    "                 # img_size=(),\n",
    "                 in_channels=4,\n",
    "                 feature_size=48,\n",
    "                 seg_out_channels=3,\n",
    "                 use_checkpoint=True,\n",
    "                 text_embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = SwinUNETR(\n",
    "            # img_size=img_size,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            feature_size=feature_size,\n",
    "            use_checkpoint=use_checkpoint)\n",
    "\n",
    "        # project BioBERT 768 â†’ Swin feature dim (48)\n",
    "        self.text_proj = nn.Linear(text_embed_dim, feature_size)\n",
    "\n",
    "        # fuse image+text channels: (2C â†’ C)\n",
    "        self.fusion = nn.Conv3d(feature_size * 2, feature_size, kernel_size=1)\n",
    "\n",
    "        # output head\n",
    "        self.seg_head = nn.Conv3d(feature_size, seg_out_channels, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, text_feature):\n",
    "    \n",
    "        # --- Image Backbone ---\n",
    "        img_features = self.backbone(x)  # [B, C, D, H, W]\n",
    "        # print(f\"[Backbone] img_features: {img_features.shape}\")\n",
    "\n",
    "        # --- Text Processing ---\n",
    "        text_feature = text_feature.mean(dim=1)\n",
    "        # print('check shape:',text_feature.shape)\n",
    "        text_feat = self.text_proj(text_feature)          # [B, 768] â†’ [B, C]\n",
    "        # print(f\"[TextProj] text_feat after Linear: {text_feat.shape}\")\n",
    "\n",
    "        text_feat = text_feat.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        # print(f\"[TextProj] text_feat after unsqueeze: {text_feat.shape}\")\n",
    "\n",
    "        text_feat = text_feat.expand_as(img_features)     # [B, C, D, H, W]\n",
    "        # print(f\"[TextProj] text_feat after expand_as: {text_feat.shape}\")\n",
    "\n",
    "        fused = torch.cat([img_features, text_feat], dim=1)  # [B, 2C, D, H, W]\n",
    "        # print(f\"[Fusion] concatenated: {fused.shape}\")\n",
    "\n",
    "        fused = self.fusion(fused)                        # [B, C, D, H, W]\n",
    "        # print(f\"[Fusion] after Conv3d fusion: {fused.shape}\")\n",
    "        \n",
    "        # --- Head ---\n",
    "        seg_output = self.seg_head(fused)\n",
    "        # print(f\"[SegHead] seg_output: {seg_output.shape}\")\n",
    "\n",
    "\n",
    "        return seg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27f5f39c-3b0b-44b6-acdc-f7998448b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # === Step 2: Set up device ===\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # === Step 3: Initialize model ===\n",
    "\n",
    "    model = MultiTaskSwinUNETR_image_text_fusion(\n",
    "       # img_size=(),\n",
    "       in_channels=4,\n",
    "       seg_out_channels=3,      # tumor classes\n",
    "       feature_size=48\n",
    "   )\n",
    "\n",
    "    # print(model) \n",
    "       \n",
    "    # === Step 4: parameter setup ===\n",
    "    start_epoch=1\n",
    "    max_epochs=200\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)\n",
    "    \n",
    "   \n",
    "    # === Step 5: Call train() ===\n",
    "    train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        max_epochs=max_epochs,\n",
    "        directory_name=directory_name,\n",
    "        start_epoch=start_epoch,\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "519e5bfc-89ab-4367-8599-ce8510120191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, input_dir, results_dir):\n",
    "    import os\n",
    "    import nibabel as nib\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from functools import partial\n",
    "    from monai.inferers import sliding_window_inference\n",
    "    from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "    from monai.utils.enums import MetricReduction\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    dice_metric = DiceMetric(\n",
    "        include_background=True,\n",
    "        reduction=MetricReduction.MEAN_BATCH,\n",
    "        get_not_nans=True\n",
    "    )\n",
    "\n",
    "    hd95_metric = HausdorffDistanceMetric(\n",
    "        include_background=True,\n",
    "        reduction=MetricReduction.MEAN_BATCH,\n",
    "        percentile=95.0\n",
    "    )\n",
    "\n",
    "    # predictor with text (keep your logic)\n",
    "    predictor_with_text = lambda x: model(x, text)\n",
    "\n",
    "    model_inferer_with_text = partial(\n",
    "        sliding_window_inference,\n",
    "        roi_size=[96,96,96],\n",
    "        sw_batch_size=1,\n",
    "        predictor=predictor_with_text,\n",
    "        overlap=0.5,\n",
    "        mode=\"gaussian\"\n",
    "    )\n",
    "    # --- RESET metrics BEFORE evaluation ---\n",
    "    dice_metric.reset()\n",
    "    hd95_metric.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            subject_id = batch[\"subject_id\"][0]\n",
    "            img = batch[\"img\"].to(device)\n",
    "            gt=batch[\"seg\"].to(device)\n",
    "            text = batch[\"text_feature\"].to(device)\n",
    "            print('check shape first',img.shape,text.shape,gt.shape)\n",
    "    \n",
    "            # --- Sliding window inference ---\n",
    "            logits = model_inferer_with_text(img)\n",
    "            # print('check:',logits)\n",
    "            \n",
    "            pred_prob = torch.sigmoid(logits)\n",
    "            pred = (pred_prob > 0.5).int()\n",
    "            # print('check:',pred,logits)\n",
    "            # --- Accumulate metrics ---\n",
    "            dice_metric(y_pred=pred, y=gt)\n",
    "            hd95_metric(y_pred=pred, y=gt)\n",
    "        \n",
    "    \n",
    "    # --- Aggregate ONCE ---\n",
    "    print('final:',dice_metric,hd95_metric)\n",
    "    dice, not_nans = dice_metric.aggregate()\n",
    "    hd95 = hd95_metric.aggregate()\n",
    "    \n",
    "   \n",
    "    print(\"Before NaN/Inf fix:\", hd95)\n",
    "\n",
    "    # --- Option C: NaN / Inf-safe HD95 (ET-safe) ---\n",
    "    hd95 = torch.nan_to_num(hd95, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    print(\"After NaN/Inf fix:\", hd95)\n",
    "    \n",
    "    dice = dice.cpu().numpy()\n",
    "    hd95 = hd95.cpu().numpy()\n",
    "    \n",
    "    print(\n",
    "        f\"Dataset Avg Dice -> \"\n",
    "        f\"TC={dice[0]:.4f}, WT={dice[1]:.4f}, ET={dice[2]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Dataset Avg HD95 -> \"\n",
    "        f\"TC={hd95[0]:.2f}, WT={hd95[1]:.2f}, ET={hd95[2]:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81140964-1be2-4de4-b105-d62b1ffbdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################test###################################\n",
    "model = MultiTaskSwinUNETR_image_text_fusion(\n",
    "       # img_size=(256, 256, 160),\n",
    "       in_channels=4,\n",
    "       seg_out_channels=3,  \n",
    "          recon_out_channels=4, \n",
    "       feature_size=48\n",
    "   )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "checkpoint_path = os.path.join(directory_name, \"best_model.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "print(\"Checkpoint loaded.\")\n",
    "\n",
    "# val_loader = load_data_validation(directory_name)\n",
    "# print('reached to val_loader')\n",
    "    # Run inference and save predictions\n",
    "with torch.no_grad():\n",
    "    test(test_loader, model,directory_name, output_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0667e-0034-4983-b843-1f9d38eba16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (monai_clean)",
   "language": "python",
   "name": "monai_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
