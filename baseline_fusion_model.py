"""
Created on Mon Jan  5 12:57:05 2026

@authors: mumuaktar, dscarmo
"""
# Standard library imports
import os

# Third-party library imports
import torch
import torch.nn as nn
from monai.networks.nets import SwinUNETR
from torch.optim.lr_scheduler import CosineAnnealingLR
from torchinfo import summary
from tqdm import tqdm

# Local module imports
from data_loader import load_data
from load_config import get_config_args
from train_function import train
from transforms_function import train_transforms, val_transforms


class SwinUNETR_image_text_fusion(nn.Module):
    """SwinUNETR_image_text_fusion model for multimodal brain tumor segmentation"""
    def __init__(self,
                 in_channels: int = 4,
                 feature_size: int = 48,
                 seg_out_channels: int = 3,
                 use_checkpoint: bool = True,
                 text_embed_dim: int = 768,
                 debug: bool = False):
        """
        Initialize the SwinUNETR_image_text_fusion model
        Args:
            in_channels: int, number of input channels (4 for MRI modalities)
            feature_size: int, number of features in the SwinUNETR backbone
            seg_out_channels: int, number of output channels (3 for WT, TC, ET)
            use_checkpoint: bool, use checkpointing for the SwinUNETR backbone
            text_embed_dim: int, number of features in the text embedding
            debug: bool, print debug messages if True
        """
        super().__init__()

        self.debug = debug

        # Use full SwinUNETR backbone, note we are using the encoder AND the decoder.
        self.backbone = SwinUNETR(
            in_channels=in_channels,
            out_channels=feature_size,
            feature_size=feature_size,  # feature_size = C (channels)
            use_checkpoint=use_checkpoint)

        # project BioBERT 768 embeddings → Swin feature_size (C)
        self.text_proj = nn.Linear(text_embed_dim, feature_size)

        # fuse image+text channels: (2*C --> C)
        self.fusion = nn.Conv3d(feature_size * 2, feature_size, kernel_size=1)

        # output head: C --> seg_out_channels (3 for WT, TC, ET)
        self.seg_head = nn.Conv3d(feature_size, seg_out_channels, kernel_size=1)

    def print_debug(self, message: str):
        """Utility to print debug messages if debug mode is enabled"""
        if self.debug:
            print(message)
        return

    def forward(self, x: torch.Tensor, text_feature: torch.Tensor) -> torch.Tensor:
        """Forward pass for the SwinUNETR_image_text_fusion model
        Args:
            x: torch.Tensor, input image tensor of shape [B, C, D, H, W]
            text_feature: torch.Tensor, text embedding tensor of shape [B, 768]
        Returns:
            seg_output: torch.Tensor, segmentation output tensor of shape [B, C, D, H, W]

        img_features generated by SwinUNETR are concatenated with text_feat to form a single tensor of shape [B, 2C, D, H, W].
        This tensor is then passed through a 1x1x1 convolution to fuse the image and text features.
        The resulting tensor is passed through a segmentation head to produce the segmentation output.
        """
        # --- Image Backbone ---
        img_features = self.backbone(x)  # [B, C, D, H, W]
        self.print_debug(f"[Backbone] img_features: {img_features.shape}")

        # --- Text Processing, compressing text embedding to C channels ---
        text_feature = text_feature.mean(dim=1)
        self.print_debug(f"check shape: {text_feature.shape}")
        text_feat: torch.Tensor = self.text_proj(text_feature)          # [B, 768] → [B, C]
        self.print_debug(f"[TextProj] text_feat after Linear: {text_feat.shape}")

        # Expand text_feat to match img_features shape for broadcasting
        text_feat = text_feat.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)   # [B, C, 1, 1, 1]
        self.print_debug(f"[TextProj] text_feat after unsqueeze: {text_feat.shape}")
        text_feat = text_feat.expand_as(img_features)     # [B, C, D, H, W]
        self.print_debug(f"[TextProj] text_feat after expand_as: {text_feat.shape}")

        fused = torch.cat([img_features, text_feat], dim=1)  # [B, 2C, D, H, W]
        self.print_debug(f"[Fusion] concatenated: {fused.shape}")

        fused: torch.Tensor = self.fusion(fused)                        # [B, C, D, H, W]
        self.print_debug(f"[Fusion] after Conv3d fusion: {fused.shape}")

        # --- Segmentation Head ---
        seg_output: torch.Tensor = self.seg_head(fused)
        self.print_debug(f"[SegHead] seg_output: {seg_output.shape}")

        return seg_output


def initialize_data_loaders(config: dict):
    """
    Initialize data loaders for training and validation
    
    Args:
        config: Dictionary containing configuration parameters
    """
    train_loader = load_data(
        base_path=config['data_dir'],
        split="train",
        transforms=train_transforms,
        batch_size=config['batch_size'],
        shuffle=True,
    )

    val_loader = load_data(
        base_path=config['data_dir'],
        split="val",
        transforms=val_transforms,
        batch_size=config['test_batch_size'],
        shuffle=False,
    )

    # Debug dataloaders
    for batch in tqdm(train_loader, desc="Testing training data loading"):
        debug_message = f"Training batch shape: {batch['text_feature'].shape}, {batch['img'].shape}, {batch['seg'].shape}"
        tqdm.write(debug_message)
        # only print first batch if not in debug mode
        if not config['debug']: 
            break

    for batch in tqdm(val_loader, desc="Testing validation data loading"):
        debug_message = f"Validation batch shape: {batch['text_feature'].shape}, {batch['img'].shape}, {batch['seg'].shape}"
        tqdm.write(debug_message)
        # only print first batch if not in debug mode
        if not config['debug']: 
            break
    
    return train_loader, val_loader

def main(config: dict):
    """
    Main entry point for training

    Three main steps: 
    1. Initialize model
    2. Initialize optimizer and learning rate scheduler
    3. Initialize data and call train()
    
    Args:
        config: Dictionary containing configuration parameters
    """
    # === Initialize model ===
    model = SwinUNETR_image_text_fusion(
       in_channels=4,
       seg_out_channels=3,      # tumor classes
       feature_size=config['feature_size']
   )

    # Print architectural summary
    print(summary(model))

    # === Training optimizer and learning rate scheduler setup ===
    max_epochs = config['max_epochs']
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)
    scheduler = CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)


    # === Initialize data and call train() ===
    train_loader, val_loader = initialize_data_loaders(config)
    train(train_loader, val_loader, model, optimizer, scheduler, config)


if __name__ == "__main__":
    # Get configuration from command line arguments and config file
    config = get_config_args(
        description="Swin UNETR for Automated Brain Tumor Segmentation",
        example_usage="Example: python baseline_fusion_model.py configs/train_config.yaml",
        default_config="configs/train_config.yaml"
    )

    # Call main function
    main(config)
